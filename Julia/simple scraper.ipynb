{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's make it first without imports just to see if it works\n",
    "workspace()\n",
    "type Parser\n",
    "    baseUrl::Int64\n",
    "    links::Array{Int64}\n",
    "end\n",
    "\n",
    "\n",
    "function handle_starttag(parser::Parser, tag, attrs)\n",
    "    if tag =='a'\n",
    "        for (key, value) in attrs\n",
    "            if key=='href'\n",
    "                newUrl = parse.urljoin(baseUrl, value)\n",
    "                append!(links, newUrl)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function getLinks(parser::Parser, url)\n",
    "    response = urlopen(parser.baseUrl) # almost certanily not best place to put it\n",
    "    if response.getheader('Content-Type') == 'test/html'\n",
    "        htmlBytes = response.read()\n",
    "        htmlString = htmlBytes.decode(\"utf-8\")\n",
    "        feed(htmlString)\n",
    "        return htmlString, links\n",
    "    else\n",
    "        return \"\",[]\n",
    "    end\n",
    "end\n",
    "#what does thsi even do!? basically we need to figure out the logic by ourself to be honest\n",
    "#it doesn't even make sense. we'll try to copy the python in julia though and hope for the best\n",
    "\n",
    "function spider(url, word, maxPages)\n",
    "    pagesToVisit = url\n",
    "    num_visisted = 0\n",
    "    foundWord = false\n",
    "end\n",
    "\n",
    "# basically we just need to have really efficient datastructures and stuff handle the parsing\n",
    "#but we straight up don't. this seems like the sort of task that julia should be realy good at though\n",
    "# loop handling and adding stuff to mutable lists. stuff like that\n",
    "# julia should be great at it, as it's just a mega recursive list with very defined types\n",
    "#shouldn't be difficult at all really\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function crawl()\n",
    "    #dagnabbit I hate julia and I'm not confident at all in it. and it has really cryptic errors\n",
    "    #compared to python. but it's a lot faster still very easy to develop in\n",
    "    #and also vastly faster and cooler than python which is pretty boring\n",
    "    #doing a simple web crawler in julia should be a fun little project\n",
    "    #and it doesn't seem to exist so it could be a fun project also\n",
    "    # in our spare time\n",
    "    \n",
    "    #so whhat do we need this crawler to do, just somehow parse the text, check for things, and hope for the bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, let's actually try and do this my own way, so we know it makes sense and we can figure it out\n",
    "# let's figure out just what a web scraper actually has to do here. it shouldn't be THAT hard\n",
    "# we just need a start url, a list of all websites taken, a list of all websites still to go, and a dictionary of urls to content\n",
    "# that shouldn't be so difficult really. oh ewll, not too hard\n",
    "\n",
    "#imports\n",
    "using Requests\n",
    "#whatever else?\n",
    "using HTTP\n",
    "using Gumbo\n",
    "using AbstractTrees\n",
    "using ArgParse\n",
    "\n",
    "type Crawler\n",
    "    startUrl::AbstractString\n",
    "    urlsVisited::Array{AbstractString}\n",
    "    urlsToCrawl::Array{AbstractString}\n",
    "    content::Dict{AbstractString, AbstractString}\n",
    "    #the content is dictoianry{url: html content}\n",
    "    breadthFirst::Bool\n",
    "    \n",
    "    #constructors\n",
    "    function Crawler(starturl::AbstractString)\n",
    "        return new(starturl, AbstractString[], AbstractString[],Dict{AbstractString, AbstractString}(), true)\n",
    "    end\n",
    "    function Crawler(starturl::AbstractString, breadthfirst::Bool)\n",
    "        return new(starturl, AbstractString[],AbstractString[],Dict{AbstractString, AbstractString}(), breadthfirst)\n",
    "    end\n",
    "    function Crawler(starturl::AbstractString, urlstocrawl::Array{AbstractString},breadthfirst::Bool)\n",
    "        return new(starturl, AbstractString[], urlstocrawl, Dict{AbstractString, AbstractString}(), breadthfirst) \n",
    "    end\n",
    "    function Crawler(starturl::AbstractString, urlstocrawl::Array{AbstractString})\n",
    "        return new(starturl, AbstractString[], urlstocrawl, Dict{AbstractString, AbstractString}(), true)\n",
    "    end\n",
    "    #remove this, just a test\n",
    "    function Crawler(urlstocrawl::Array{AbstractString}, breadthfirst::Bool)\n",
    "        return new(\"\", AbstractString[], urlstocrawl, Dict{AbstractString, AbstractString}(), breadthfirst)\n",
    "    end\n",
    "    function Crawler(urlstocrawl::Array{AbstractString})\n",
    "        return new(\"\", AbstractString[], urlstocrawl, Dict{AbstractString, AbstractString}(), true)\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "print(\"sorted!\")\n",
    "\n",
    "# okay, that's us kind of sorted. nowe we shuold get some stuff. let's just run this, see if it works lol\n",
    "#that gives us a huge list of errors I don't understand at all... oh well. that's pretty unfortunate really\n",
    "# so it goes, I suppose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, now for our actual functoins. let's first write this as a crazy monolithic one so we understand\n",
    "#we also need a monolithic one so we can figure out how it works\n",
    "#and write sufficient constructors, so let's do that\n",
    "#let's try creating some crawlers\n",
    "crawler = Crawler(\"http://google.com\")\n",
    "const SuccessCode=200\n",
    "#crawler = Crawler()\n",
    "# okay, creation works. that's good. now let's get our function with the mega loop\n",
    "#with our default num_iterations number. that's not going to be too tricky, hopefully\n",
    "function crawl(crawler::Crawler, num_iterations::Integer=10, verbose=true, save=true)\n",
    "    #first we check if it's the first thing we see. so we should just check this\n",
    "    #shall we just define variables from the crawler? nah, let's not. we should just access them consistently\n",
    "    #as it's meant t be updated in place, I assume\n",
    "    #we should dump this in thefucntoin so it doesn't slow down\n",
    "    const successCode = 200\n",
    "    \n",
    "    #our immediate return if correct\n",
    "    if isempty(crawler.urlsToCrawl) && crawler.startUrl==\"\"\n",
    "        return crawler.content, crawler.urlsVisited\n",
    "    end\n",
    "    \n",
    "\n",
    "    if isempty(crawler.urlsToCrawl) && crawler.startUrl!=\"\"\n",
    "        #so we are at the beginning so we visit our first piece\n",
    "        #we set the starturl to urls to crawl\n",
    "        push!(crawler.urlsToCrawl,crawler.startUrl)\n",
    "        crawler.startUrl=\"\"\n",
    "    end\n",
    "    \n",
    "    #okay, now we begin the loop\n",
    "    for i in 0:num_iterations\n",
    "        #we check if empty we probably shouldn't do this on each iteratino, but oh well!\n",
    "        if isempty(crawler.urlsToCrawl) && crawler.startUrl==\"\"\n",
    "            return crawler.content, crawler.urlsVisited\n",
    "        end\n",
    "        url = pop!(crawler.urlsToCrawl)\n",
    "        #we get the content\n",
    "        #we make the request with http\n",
    "        #we first check this works... idk\n",
    "        #println(crawler.urlsVisited)\n",
    "        #println(crawler.urlsToCrawl)\n",
    "        if !(url in crawler.urlsVisited)\n",
    "\n",
    "            if verbose==true\n",
    "                println(\"requesting $url\")\n",
    "            end \n",
    "            try\n",
    "                \n",
    "                response = HTTP.get(url)\n",
    "                #println(response)\n",
    "                #check success code and procede if correct\n",
    "                if response.status==successCode\n",
    "                    # okay, here's what we do. we do our parsing string here\n",
    "                    res = String(response.body)\n",
    "                    doc = parsehtml(res)\n",
    "                    if verbose == true\n",
    "                        println(\"response received and is successful\")\n",
    "                    end\n",
    "\n",
    "                    #if we succeed we update our links\n",
    "                    crawler.content[url] = res\n",
    "\n",
    "                    #print(typeof(url))\n",
    "                   # println(\"\")\n",
    "                   # println(\"type of crawler.urlsvisited \", typeof(crawler.urlsVisited))\n",
    "                   # println(\"url: \", url)\n",
    "                   # println(crawler.urlsVisited)\n",
    "                    push!(crawler.urlsVisited, url)\n",
    "\n",
    "                    #we go through all elements get links\n",
    "                    for elem in PostOrderDFS(doc.root)\n",
    "                        if typeof(elem) == Gumbo.HTMLElement{:a}\n",
    "                            link=get(elem.attributes, \"href\",\"#\")\n",
    "                            if link != \"#\"\n",
    "                                #then it's succeeded we have link\n",
    "                               # println(typeof(link))\n",
    "                                push!(crawler.urlsToCrawl, link)\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "                if url in crawler.urlsToCrawl\n",
    "                    println(\"repeat url\")\n",
    "                    num_iterations +=1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #now once we're finished our loop\n",
    "    #we return stuff and save\n",
    "    \n",
    "    if save==true\n",
    "        #we save the files somewhere\n",
    "    end\n",
    "    return crawler.content, crawler.urlsVisited\n",
    "end\n",
    "\n",
    "# okay, yes! we actually have http parsing strings working. that's just so awesome! we can do this\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our arg parse settings so we can see if it works and write it up properly\n",
    "\n",
    "function parse_commandline()\n",
    "\n",
    "    s = ArgParseSettings(prog=\"Julia Web Crawler\",\n",
    "                        description=\"A webcrawler written in Julia\",\n",
    "                        commands_are_required=false,\n",
    "                        version=\"0.0.1\",\n",
    "                        add_version=true)\n",
    "\n",
    "    @add_arg_table s begin\n",
    "            \"--urls\"\n",
    "                help=\"either a url to start at or a set of urls to start visiting\"\n",
    "                required=true\n",
    "            \"--breadth-first\", \"--b\"\n",
    "                help=\"a flag or whether the crawler should search depth or breadth first\"\n",
    "                action=:store_true\n",
    "                default = true\n",
    "            \"--num_iterations\", \"--i\"\n",
    "                help=\"the number of iteratinos to run the crawler for\"\n",
    "                default=10\n",
    "    end\n",
    "    return parse_args(s)\n",
    "end\n",
    "\n",
    "function setupCrawler(urls, b=true)\n",
    "    return Crawler(urls, b)\n",
    "end\n",
    "\n",
    "# at some point we should add functionality like a place to put stuff, or idk really\n",
    "# who even knows\n",
    "#not sure where we should put save files also tbh\n",
    "#as that seems quite important really\n",
    "#and idk\n",
    "#and a scheduler for having multiple parallel crawlers, as they don't need to interact\n",
    "#julia should be good at that also hopefully!\n",
    "\n",
    "\n",
    "function main()\n",
    "    #parsed_args= parse_commandline()\n",
    "    #now we get our args\n",
    "   # urls = parsed_args[\"urls\"]\n",
    "    #breadth_first = parsed_args[\"breadth-first\"]\n",
    "    #we generate thecrawler, and then roll, baby\n",
    "    #num_iterations = parsed_args[\"num_iterations\"]\n",
    "    \n",
    "    #obviously in the notebook we have no argumentsto parse, so we just do this\n",
    "    urls=\"http://stackoverflow.com\"\n",
    "    breadth_first=true\n",
    "    crawler= Crawler(urls, breadth_first)\n",
    "    num_iterations = 20\n",
    "    crawl(crawler, num_iterations)\n",
    "end\n",
    "\n",
    "#main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "if a ==2\n",
    "    print(\"yes\")\n",
    "end\n",
    "\n",
    "for i in 1:10\n",
    "    print(i)\n",
    "    print(typeof(i))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"HTTP\")\n",
    "using HTTP\n",
    "\n",
    "url=\"https://stackoverflow.com/questions/37360340/how-to-pass-dict-as-the-argument-to-method-julia\"\n",
    "response = HTTP.get(url)\n",
    "print(response.body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
